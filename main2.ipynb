{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/e/ws/2024/S2T/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightning as L\n",
    "import torchaudio.transforms as T\n",
    "from transformers import Wav2Vec2Model\n",
    "from module.KAN import KAN\n",
    "from metrix.LM import LandmarkDistance, LandmarkVelocityDifference\n",
    "import torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import numpy as np\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "\n",
    "class MEAD(Dataset):\n",
    "    def __init__(self, data_path, duration, batch):\n",
    "        super(MEAD, self).__init__()\n",
    "\n",
    "        with open(data_path,\"r\") as f:\n",
    "            datalist = json.load(f)\n",
    "        ln = int(len(datalist)/batch)*batch\n",
    "        self.datalist = datalist[:ln]\n",
    "        self.time = duration\n",
    "        self.mn = 0\n",
    "        self.mx = 256\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datalist)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.datalist[idx]\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(f\"dataset/duration/audios/{row['audio']}\", num_frames= 48000* self.time, normalize=\"True\")\n",
    "        landmark =  torch.from_numpy(np.load(f\"dataset/duration/fa_landmarks/{row['landmark']}\")).permute(0,2,1)[:30*self.time]\n",
    "        landmark = (landmark - self.mn) / (self.mx - self.mn)\n",
    "        # ilm = torch.mean(landmark, dim=1)\n",
    "        name = row[\"name\"].split('_')[0]\n",
    "        ilm = landmark[0]\n",
    "        waveform = T.Resample(48000, 16000, dtype=waveform.dtype)(waveform)\n",
    "        waveform = torch.mean(waveform, dim=0)\n",
    "\n",
    "        # waveform = torch.load(f\"dataset/vec2s/M003_front_{row['name']}.pt\")\n",
    "\n",
    "        return {'name': row['name'], 'audio':waveform.float(), 'target':landmark.float(), 'ilm':ilm.float(), 'label':row['emotion']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import pytorch_lightning as L\n",
    "\n",
    "class Transformer(L.LightningModule):\n",
    "    def __init__(self, in_c, out_c, nhead, num_lay=2, sub_chanel=49,):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "        en_lay = nn.TransformerEncoderLayer(d_model=in_c, nhead=nhead, dropout=0.2)\n",
    "        self.transformer_encoder = nn.Sequential(\n",
    "            nn.TransformerEncoder(en_lay, num_layers=num_lay),\n",
    "            nn.Linear(in_features=in_c, out_features=out_c*2)\n",
    "        )\n",
    "        de_lay = nn.TransformerDecoderLayer(d_model=out_c*2, nhead=nhead, dropout=0.2)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(de_lay, num_layers=num_lay)\n",
    "        self.conv = nn.Conv1d(1, sub_chanel, 1,1)\n",
    "\n",
    "    def forward(self, x, v):\n",
    "        x = self.transformer_encoder(x)\n",
    "        v = self.conv(v)\n",
    "        x = self.transformer_decoder(v, x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class S2LM(L.LightningModule):\n",
    "    def __init__(self, batch, init_lr, num_of_landmarks=478):\n",
    "        super(S2LM, self).__init__()\n",
    "\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "        self.MOUTH_LANDMARKS = [49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 48]\n",
    "        self.init_lr = init_lr\n",
    "        self.batch = batch\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.LD = LandmarkDistance()\n",
    "        self.LVD = LandmarkVelocityDifference()\n",
    "        self.w2v = torchaudio.pipelines.WAV2VEC2_BASE.get_model()\n",
    "        self.w2v.eval()\n",
    "        for param in self.w2v.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.trans_ct = Transformer(in_c=768, out_c=num_of_landmarks, nhead=1, num_lay=1)\n",
    "        # self.trans_gb = Transformer(in_c=768, out_c=num_of_landmarks, nhead=2, num_lay=1)\n",
    "        # self.trans_gbm = Transformer(in_c=768, out_c=len(self.MOUTH_LANDMARKS), nhead=2, num_lay=1)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=768, hidden_size=68, num_layers=1, batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        self.mouth = nn.LSTM(input_size=768, hidden_size=len(self.MOUTH_LANDMARKS), num_layers=1, batch_first=True, bidirectional=True, dropout=0.3)\n",
    "\n",
    "        self.R1 = nn.Sequential(\n",
    "            nn.Conv2d(98, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 30, 3, 1, 1)\n",
    "        )\n",
    "        self.R2 = nn.Sequential(nn.Conv2d(98, 30, 1, 1))\n",
    "                                \n",
    "        self.R3 = nn.Sequential(\n",
    "            nn.Conv2d(49, 128, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 30, 3, 1, 1)\n",
    "        )\n",
    "        self.KAN = KAN([68, 512, 68])\n",
    "\n",
    "    def forward(self, x, v, w):\n",
    "\n",
    "        x = self.w2v(x)[0]\n",
    "        vm = v[:,:,self.MOUTH_LANDMARKS].view(self.batch, 1, -1)\n",
    "        v = v.view(self.batch, 1, -1)\n",
    "        # gbm = self.trans_gbm(x, vm)\n",
    "        # gb = self.trans_gb(x, v)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            w = nn.functional.softmax(w.float(), dim=0)\n",
    "            w = w[:, None, None]\n",
    "            x = torch.mul(x, w)\n",
    "\n",
    "        f = self.lstm(x)[0]\n",
    "        m = self.mouth(x)[0]\n",
    "        x = self.trans_ct(x, v)\n",
    "\n",
    "        x = torch.cat([f, x], dim=1)\n",
    "        m = torch.cat([m], dim=1)\n",
    "\n",
    "        x = x.view(self.batch, -1, 2, 68)\n",
    "        m = m.view(self.batch, -1, 2, 20)\n",
    "\n",
    "\n",
    "        m = self.R3(m)\n",
    "        t = self.R2(x)\n",
    "        x = self.R1(x)\n",
    "        x *= t\n",
    "        x[:, :, :, self.MOUTH_LANDMARKS] = m\n",
    "        y = self.KAN(x)\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch['audio']\n",
    "        y = batch['target']\n",
    "        v = batch['ilm']\n",
    "        w = batch['label']\n",
    "        y_ = self.forward(x, v, w)\n",
    "        tt_loss = self.loss(y_.float(), y.float())\n",
    "        m_loss = self.loss(y_[:, :, :, self.MOUTH_LANDMARKS].float(), y[:, :, :, self.MOUTH_LANDMARKS].float())\n",
    "        loss = tt_loss * 0.2 + m_loss * 0.8\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch['audio']\n",
    "        y = batch['target']\n",
    "        v = batch['ilm']\n",
    "        w = batch['label']\n",
    "\n",
    "        y_ = self.forward(x, v, w)\n",
    "        loss = self.loss(y_.float(), y.float())\n",
    "        ld = self.LD(y_, y)\n",
    "        lvd = self.LVD(y_, y)\n",
    "        ldm = self.LD(y_[:,:, :,self.MOUTH_LANDMARKS], y[:,:, :, self.MOUTH_LANDMARKS])\n",
    "        lvdm = self.LVD(y_[:,:, :,self.MOUTH_LANDMARKS], y[:,:, :, self.MOUTH_LANDMARKS])\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('ld', ld, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('lvd', lvd, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('ldm', ldm, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('lvdm', lvdm, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch['audio']\n",
    "        y = batch['target']\n",
    "        v = batch['ilm']\n",
    "        w = batch['label']\n",
    "\n",
    "        y_ = self.forward(x, v, w)\n",
    "        loss = self.loss(y_.float(), y.float())\n",
    "        ld = self.LD(y_, y)\n",
    "        lvd = self.LVD(y_, y)\n",
    "        ldm = self.LD(y_[:,:, :,self.MOUTH_LANDMARKS], y[:,:, :, self.MOUTH_LANDMARKS])\n",
    "        lvdm = self.LVD(y_[:,:, :,self.MOUTH_LANDMARKS], y[:,:, :, self.MOUTH_LANDMARKS])\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('ld', ld, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('lvd', lvd, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('ldm', ldm, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('lvdm', lvdm, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.init_lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.00001)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss_epoch'\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/e/ws/2024/S2T/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/mnt/e/ws/2024/S2T/.venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "datas = MEAD(\"dataset/duration/datalist_lv3.json\", duration=1, batch=16)\n",
    "trainsize = int(len(datas) * (90 /100) / 16) * 16\n",
    "testsize = len(datas) - trainsize\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(datas, [trainsize, testsize])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True,  num_workers=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False,  num_workers=16)\n",
    "\n",
    "model = S2LM(batch=16, init_lr=3e-4, num_of_landmarks=68)\n",
    "# model = S2LM.load_from_checkpoint(\",batch=16, init_lr=3e-4, num_of_landmarks=68)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/e/ws/2024/S2T/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type          | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | loss     | MSELoss       | 0      | train\n",
      "1 | w2v      | Wav2Vec2Model | 94.4 M | eval \n",
      "2 | trans_ct | Transformer   | 6.3 M  | train\n",
      "3 | lstm     | LSTM          | 455 K  | train\n",
      "4 | mouth    | LSTM          | 126 K  | train\n",
      "5 | R1       | Sequential    | 360 K  | train\n",
      "6 | R2       | Sequential    | 3.0 K  | train\n",
      "7 | R3       | Sequential    | 107 K  | train\n",
      "8 | KAN      | KAN           | 696 K  | train\n",
      "---------------------------------------------------\n",
      "8.1 M     Trainable params\n",
      "94.4 M    Non-trainable params\n",
      "102 M     Total params\n",
      "409.796   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 448/448 [00:25<00:00, 17.42it/s, v_num=5, train_loss_step=7.69e-5, val_loss=0.000262, ld=4.130, lvd=6.020, ldm=2.120, lvdm=1.370, train_loss_epoch=0.000114] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 448/448 [00:28<00:00, 15.85it/s, v_num=5, train_loss_step=7.69e-5, val_loss=0.000262, ld=4.130, lvd=6.020, ldm=2.120, lvdm=1.370, train_loss_epoch=0.000114]\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")\n",
    "trainer = L.Trainer(max_epochs=200, default_root_dir=\"weights\", fast_dev_run=False, check_val_every_n_epoch=1)\n",
    "trainer.fit(model, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model.to(\"cuda\")\n",
    "# model.eval()\n",
    "model = S2LM.load_from_checkpoint(\"weights/lightning_logs/240625*_500/checkpoints/epoch=499-step=715500.ckpt\", batch=16, init_lr=1e-3, num_of_landmarks=68)\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 159/159 [00:07<00:00, 21.94it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "           ld                2.016840934753418\n",
      "           ldm              1.1927270889282227\n",
      "           lvd              2.1205756664276123\n",
      "          lvdm              0.9523807764053345\n",
      "        val_loss           6.27173503744416e-05\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 6.27173503744416e-05,\n",
       "  'ld': 2.016840934753418,\n",
       "  'lvd': 2.1205756664276123,\n",
       "  'ldm': 1.1927270889282227,\n",
       "  'lvdm': 0.9523807764053345}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = L.Trainer()\n",
    "trainer.test(model, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# MOUTH_LANDMARKS =[61, 76, 62, 185, 184, 183, 78, 77, 146, 191, 95, 96, 40, 74, 42, 80, 88, 89, 90, 91, 39, 73, 41, 81, 178, 179, \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#                                180, 181, 37, 72, 38, 82, 87, 86, 85, 84, 0, 11, 12, 13, 14, 15, 16, 17, 267, 302, 268, 312, 317, 316, 315, 314,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#                                  269, 303, 271, 311, 402, 403, 404, 405, 270, 304, 272, 310, 318, 319, 320, 321, 409, 408, 407, 415, 324, 325,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# datas = MEAD(\"dataset/eachsec/M030_sf/all.json\", batch=64, stage='test')\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# dataloader = DataLoader(datas, batch_size=64, shuffle=False,  num_workers=8)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m MOUTH_LANDMARKS \u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m49\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m51\u001b[39m, \u001b[38;5;241m52\u001b[39m, \u001b[38;5;241m53\u001b[39m, \u001b[38;5;241m54\u001b[39m, \u001b[38;5;241m55\u001b[39m, \u001b[38;5;241m56\u001b[39m, \u001b[38;5;241m57\u001b[39m, \u001b[38;5;241m58\u001b[39m, \u001b[38;5;241m59\u001b[39m, \u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m61\u001b[39m, \u001b[38;5;241m62\u001b[39m, \u001b[38;5;241m63\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m65\u001b[39m, \u001b[38;5;241m66\u001b[39m, \u001b[38;5;241m67\u001b[39m, \u001b[38;5;241m48\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mtest_dataloader\u001b[49m))\n\u001b[1;32m     13\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_dataloader))\n\u001b[1;32m     14\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_dataloader))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "# MOUTH_LANDMARKS =[61, 76, 62, 185, 184, 183, 78, 77, 146, 191, 95, 96, 40, 74, 42, 80, 88, 89, 90, 91, 39, 73, 41, 81, 178, 179, \n",
    "#                                180, 181, 37, 72, 38, 82, 87, 86, 85, 84, 0, 11, 12, 13, 14, 15, 16, 17, 267, 302, 268, 312, 317, 316, 315, 314,\n",
    "#                                  269, 303, 271, 311, 402, 403, 404, 405, 270, 304, 272, 310, 318, 319, 320, 321, 409, 408, 407, 415, 324, 325,\n",
    "#                                    307, 375, 308, 292, 306, 291]\n",
    "\n",
    "\n",
    "# datas = MEAD(\"dataset/eachsec/M030_sf/all.json\", batch=64, stage='test')\n",
    "# dataloader = DataLoader(datas, batch_size=64, shuffle=False,  num_workers=8)\n",
    "\n",
    "MOUTH_LANDMARKS =[49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 48]\n",
    "item = next(iter(test_dataloader))\n",
    "item = next(iter(test_dataloader))\n",
    "item = next(iter(test_dataloader))\n",
    "\n",
    "\n",
    "x = item['audio'].to(\"cuda\")\n",
    "v = item['ilm'].to(\"cuda\")\n",
    "y = item['target'].to(\"cuda\")\n",
    "w = item['label'].to(\"cuda\")\n",
    "name = item['name'][1].split(\"_\",1)\n",
    "video = torchvision.io.read_video(f\"dataset/duration/vidcrops/{name[0]}_front_{name[1]}.mp4\", pts_unit='sec')[0]\n",
    "\n",
    "y_ = model.forward(x,v,w)\n",
    "y_ = y_.cpu().detach().numpy()\n",
    "y = y.cpu().detach().numpy()\n",
    "v = v.cpu().detach().numpy()\n",
    "\n",
    "y = y * (datas.mx - datas.mn) + datas.mn\n",
    "y_ = y_ * (datas.mx - datas.mn) + datas.mn\n",
    "\n",
    "# y = y * (datas.mx - datas.mn) + datas.mn\n",
    "# y_ = y_ * (datas.mx - datas.mn) + datas.mn\n",
    "\n",
    "\n",
    "for t in range(0,len(y[1]),5):\n",
    "    plain = np.ones((256,256,3))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(video[t])\n",
    "    plain = np.ones((256,256,3))\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(plain)\n",
    "    plt.scatter(y[1][t][0,:], y[1][t][1,:], c='b', s=0.5)\n",
    "    plt.title(\"GT\")\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(plain)\n",
    "    plt.scatter(y_[1][t][0,:], y_[1][t][1,:], c='r', s=0.5)\n",
    "    plt.title(\"Predit\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, image2, from 'temp/out_temp/%d.png':\n",
      "  Duration: 00:00:01.20, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgba(pc), 640x480 [SAR 3937:3937 DAR 4:3], 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x56554a4f6740] using SAR=1/1\n",
      "[libx264 @ 0x56554a4f6740] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
      "[libx264 @ 0x56554a4f6740] profile High 4:4:4 Predictive, level 3.0, 4:4:4, 8-bit\n",
      "[libx264 @ 0x56554a4f6740] 264 - core 163 r3060 5db6aa6 - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=15 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'temp/video_0.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv444p(tv, progressive), 640x480 [SAR 1:1 DAR 4:3], q=2-31, 30 fps, 15360 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame=   36 fps=0.0 q=-1.0 Lsize=      29kB time=00:00:01.10 bitrate= 216.5kbits/s dup=6 drop=0 speed=10.4x    \n",
      "video:28kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 4.565408%\n",
      "[libx264 @ 0x56554a4f6740] frame I:1     Avg QP:14.26  size:  8876\n",
      "[libx264 @ 0x56554a4f6740] frame P:9     Avg QP:20.15  size:  1118\n",
      "[libx264 @ 0x56554a4f6740] frame B:26    Avg QP:21.47  size:   340\n",
      "[libx264 @ 0x56554a4f6740] consecutive B-frames:  2.8%  0.0%  8.3% 88.9%\n",
      "[libx264 @ 0x56554a4f6740] mb I  I16..4: 37.7% 49.2% 13.1%\n",
      "[libx264 @ 0x56554a4f6740] mb P  I16..4:  0.0%  0.2%  0.1%  P16..4:  2.5%  2.2%  1.3%  0.0%  0.0%    skip:93.7%\n",
      "[libx264 @ 0x56554a4f6740] mb B  I16..4:  0.0%  0.1%  0.0%  B16..8:  2.5%  0.7%  0.3%  direct: 0.1%  skip:96.2%  L0:46.5% L1:49.6% BI: 3.8%\n",
      "[libx264 @ 0x56554a4f6740] 8x8 transform intra:50.9% inter:29.4%\n",
      "[libx264 @ 0x56554a4f6740] coded y,u,v intra: 11.9% 4.4% 4.9% inter: 0.8% 0.3% 0.2%\n",
      "[libx264 @ 0x56554a4f6740] i16 v,h,dc,p: 87% 12%  1%  0%\n",
      "[libx264 @ 0x56554a4f6740] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 12%  7% 76%  1%  1%  1%  1%  1%  0%\n",
      "[libx264 @ 0x56554a4f6740] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 37% 25% 20%  2%  4%  3%  4%  3%  2%\n",
      "[libx264 @ 0x56554a4f6740] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x56554a4f6740] ref P L0: 63.6%  9.2% 17.8%  9.4%\n",
      "[libx264 @ 0x56554a4f6740] ref B L0: 79.1% 15.8%  5.1%\n",
      "[libx264 @ 0x56554a4f6740] ref B L1: 94.5%  5.5%\n",
      "[libx264 @ 0x56554a4f6740] kb/s:185.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video created: temp/video_0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, image2, from 'temp/out_temp/%d.png':\n",
      "  Duration: 00:00:01.20, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgba(pc), 640x480 [SAR 3937:3937 DAR 4:3], 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x55b561e46940] using SAR=1/1\n",
      "[libx264 @ 0x55b561e46940] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
      "[libx264 @ 0x55b561e46940] profile High 4:4:4 Predictive, level 3.0, 4:4:4, 8-bit\n",
      "[libx264 @ 0x55b561e46940] 264 - core 163 r3060 5db6aa6 - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=15 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'temp/video_1.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv444p(tv, progressive), 640x480 [SAR 1:1 DAR 4:3], q=2-31, 30 fps, 15360 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame=   36 fps=0.0 q=-1.0 Lsize=      25kB time=00:00:01.10 bitrate= 187.4kbits/s dup=6 drop=0 speed=11.7x    \n",
      "video:24kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 5.347506%\n",
      "[libx264 @ 0x55b561e46940] frame I:1     Avg QP:14.12  size: 10396\n",
      "[libx264 @ 0x55b561e46940] frame P:10    Avg QP:19.50  size:   768\n",
      "[libx264 @ 0x55b561e46940] frame B:25    Avg QP:21.18  size:   228\n",
      "[libx264 @ 0x55b561e46940] consecutive B-frames:  5.6%  5.6%  0.0% 88.9%\n",
      "[libx264 @ 0x55b561e46940] mb I  I16..4: 38.4% 48.3% 13.2%\n",
      "[libx264 @ 0x55b561e46940] mb P  I16..4:  0.0%  0.1%  0.0%  P16..4:  1.8%  1.2%  0.9%  0.0%  0.0%    skip:96.0%\n",
      "[libx264 @ 0x55b561e46940] mb B  I16..4:  0.0%  0.1%  0.0%  B16..8:  2.3%  0.4%  0.2%  direct: 0.1%  skip:96.8%  L0:40.3% L1:58.0% BI: 1.7%\n",
      "[libx264 @ 0x55b561e46940] 8x8 transform intra:49.2% inter:14.0%\n",
      "[libx264 @ 0x55b561e46940] coded y,u,v intra: 11.7% 5.4% 5.5% inter: 0.5% 0.3% 0.1%\n",
      "[libx264 @ 0x55b561e46940] i16 v,h,dc,p: 86% 12%  1%  1%\n",
      "[libx264 @ 0x55b561e46940] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu:  9%  7% 79%  1%  1%  1%  1%  1%  0%\n",
      "[libx264 @ 0x55b561e46940] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 38% 25% 18%  2%  3%  4%  4%  4%  2%\n",
      "[libx264 @ 0x55b561e46940] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x55b561e46940] ref P L0: 56.9%  4.0% 22.0% 17.1%\n",
      "[libx264 @ 0x55b561e46940] ref B L0: 74.1% 16.3%  9.6%\n",
      "[libx264 @ 0x55b561e46940] ref B L1: 93.8%  6.2%\n",
      "[libx264 @ 0x55b561e46940] kb/s:158.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video created: temp/video_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, image2, from 'temp/out_temp/%d.png':\n",
      "  Duration: 00:00:01.20, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgba(pc), 640x480 [SAR 3937:3937 DAR 4:3], 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x560b8d1fa740] using SAR=1/1\n",
      "[libx264 @ 0x560b8d1fa740] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
      "[libx264 @ 0x560b8d1fa740] profile High 4:4:4 Predictive, level 3.0, 4:4:4, 8-bit\n",
      "[libx264 @ 0x560b8d1fa740] 264 - core 163 r3060 5db6aa6 - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=15 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'temp/video_2.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv444p(tv, progressive), 640x480 [SAR 1:1 DAR 4:3], q=2-31, 30 fps, 15360 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame=   36 fps=0.0 q=-1.0 Lsize=      25kB time=00:00:01.10 bitrate= 188.6kbits/s dup=6 drop=0 speed=10.7x    \n",
      "video:24kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 5.277045%\n",
      "[libx264 @ 0x560b8d1fa740] frame I:1     Avg QP:14.13  size:  9008\n",
      "[libx264 @ 0x560b8d1fa740] frame P:10    Avg QP:19.40  size:   843\n",
      "[libx264 @ 0x560b8d1fa740] frame B:25    Avg QP:21.01  size:   260\n",
      "[libx264 @ 0x560b8d1fa740] consecutive B-frames:  2.8% 11.1%  8.3% 77.8%\n",
      "[libx264 @ 0x560b8d1fa740] mb I  I16..4: 37.8% 48.4% 13.8%\n",
      "[libx264 @ 0x560b8d1fa740] mb P  I16..4:  0.0%  0.1%  0.0%  P16..4:  2.1%  1.6%  1.2%  0.0%  0.0%    skip:95.0%\n",
      "[libx264 @ 0x560b8d1fa740] mb B  I16..4:  0.0%  0.1%  0.0%  B16..8:  2.5%  0.6%  0.3%  direct: 0.1%  skip:96.4%  L0:41.6% L1:56.2% BI: 2.2%\n",
      "[libx264 @ 0x560b8d1fa740] 8x8 transform intra:50.1% inter:24.4%\n",
      "[libx264 @ 0x560b8d1fa740] coded y,u,v intra: 11.2% 4.1% 4.4% inter: 0.6% 0.3% 0.2%\n",
      "[libx264 @ 0x560b8d1fa740] i16 v,h,dc,p: 88% 11%  1%  0%\n",
      "[libx264 @ 0x560b8d1fa740] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 14%  5% 78%  0%  1%  1%  0%  1%  1%\n",
      "[libx264 @ 0x560b8d1fa740] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 42% 23% 18%  2%  3%  4%  4%  3%  2%\n",
      "[libx264 @ 0x560b8d1fa740] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x560b8d1fa740] ref P L0: 57.5%  9.5% 19.2% 13.8%\n",
      "[libx264 @ 0x560b8d1fa740] ref B L0: 77.0% 16.4%  6.6%\n",
      "[libx264 @ 0x560b8d1fa740] ref B L1: 94.9%  5.1%\n",
      "[libx264 @ 0x560b8d1fa740] kb/s:159.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video created: temp/video_2.mp4\n",
      "Video created: temp/video_3.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, image2, from 'temp/out_temp/%d.png':\n",
      "  Duration: 00:00:01.20, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgba(pc), 640x480 [SAR 3937:3937 DAR 4:3], 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x55fcc06c7940] using SAR=1/1\n",
      "[libx264 @ 0x55fcc06c7940] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
      "[libx264 @ 0x55fcc06c7940] profile High 4:4:4 Predictive, level 3.0, 4:4:4, 8-bit\n",
      "[libx264 @ 0x55fcc06c7940] 264 - core 163 r3060 5db6aa6 - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=15 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'temp/video_3.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv444p(tv, progressive), 640x480 [SAR 1:1 DAR 4:3], q=2-31, 30 fps, 15360 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame=   36 fps=0.0 q=-1.0 Lsize=      25kB time=00:00:01.10 bitrate= 182.8kbits/s dup=6 drop=0 speed=10.7x    \n",
      "video:23kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 5.384325%\n",
      "[libx264 @ 0x55fcc06c7940] frame I:1     Avg QP:14.05  size:  9012\n",
      "[libx264 @ 0x55fcc06c7940] frame P:10    Avg QP:18.70  size:   802\n",
      "[libx264 @ 0x55fcc06c7940] frame B:25    Avg QP:21.17  size:   245\n",
      "[libx264 @ 0x55fcc06c7940] consecutive B-frames:  2.8%  5.6% 25.0% 66.7%\n",
      "[libx264 @ 0x55fcc06c7940] mb I  I16..4: 38.2% 48.2% 13.6%\n",
      "[libx264 @ 0x55fcc06c7940] mb P  I16..4:  0.0%  0.3%  0.0%  P16..4:  1.8%  1.5%  1.2%  0.0%  0.0%    skip:95.2%\n",
      "[libx264 @ 0x55fcc06c7940] mb B  I16..4:  0.0%  0.2%  0.0%  B16..8:  2.2%  0.5%  0.2%  direct: 0.1%  skip:96.7%  L0:45.2% L1:52.4% BI: 2.4%\n",
      "[libx264 @ 0x55fcc06c7940] 8x8 transform intra:51.1% inter:24.8%\n",
      "[libx264 @ 0x55fcc06c7940] coded y,u,v intra: 11.1% 4.0% 4.5% inter: 0.5% 0.2% 0.1%\n",
      "[libx264 @ 0x55fcc06c7940] i16 v,h,dc,p: 87% 12%  1%  0%\n",
      "[libx264 @ 0x55fcc06c7940] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu:  8%  7% 80%  0%  1%  1%  0%  1%  1%\n",
      "[libx264 @ 0x55fcc06c7940] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 41% 25% 16%  2%  3%  4%  4%  3%  2%\n",
      "[libx264 @ 0x55fcc06c7940] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x55fcc06c7940] ref P L0: 58.9% 11.9% 18.5% 10.8%\n",
      "[libx264 @ 0x55fcc06c7940] ref B L0: 78.2% 17.1%  4.7%\n",
      "[libx264 @ 0x55fcc06c7940] ref B L1: 94.9%  5.1%\n",
      "[libx264 @ 0x55fcc06c7940] kb/s:154.39\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    name = item['name'][i].split(\"_\",1)\n",
    "    video = torchvision.io.read_video(f\"dataset/duration/vidcrops/{name[0]}_front_{name[1]}.mp4\", pts_unit='sec')[0]\n",
    "    for t in range(len(y_[i])):\n",
    "        plain = np.ones((256,256,3))\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(video[t])\n",
    "        plain = np.ones((256,256,3))\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(plain)\n",
    "        plt.scatter(y[i][t][0,:], y[i][t][1,:], c='b', s=0.5)\n",
    "        plt.title(\"GT\")\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(plain)\n",
    "        plt.scatter(y_[i][t][0,:], y_[i][t][1,:], c='r', s=0.5)\n",
    "        plt.title(\"Predit\")\n",
    "        plt.savefig(f\"temp/out_temp/{t}.png\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    image_folder = 'temp/out_temp'  # Replace with your folder path\n",
    "    video_name = f'temp/video_{i}.mp4'\n",
    "\n",
    "    command = ['ffmpeg', '-i', f'{image_folder}/%d.png',  '-r', '30', video_name, '-y']\n",
    "    subprocess.run(command)\n",
    "    print(f\"Video created: {video_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File dataset/fa_datalist.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset/fa_datalist.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e/ws/2024/S2T/venv/lib/python3.10/site-packages/pandas/io/json/_json.py:791\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    789\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[0;32m/mnt/e/ws/2024/S2T/venv/lib/python3.10/site-packages/pandas/io/json/_json.py:904\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m filepath_or_buffer\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_from_filepath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[0;32m/mnt/e/ws/2024/S2T/venv/lib/python3.10/site-packages/pandas/io/json/_json.py:960\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    952\u001b[0m     filepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[1;32m    959\u001b[0m ):\n\u001b[0;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal json to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    967\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    968\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File dataset/fa_datalist.json does not exist"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"dataset/fa_datalist.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df['name'].str.split('_').apply(lambda x: x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = new.drop(columns=['level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.to_json('dataset/datalist_lv3.json', orient='records', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
